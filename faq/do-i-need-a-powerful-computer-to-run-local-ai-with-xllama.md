# Do I need a powerful computer to run local AI with XLlama?

That‚Äôs probably one of the first questions that comes to mind, and it‚Äôs a good one.

Here‚Äôs what‚Äôs going on under the hood: **XLlama is built on top of Ollama**, which handles the actual model loading and execution. Ollama itself uses a backend called **llama.cpp**, designed to run models locally on your machine.

So when you ask ‚Äúwhat kind of computer do I need,‚Äù the real answer is:\
**It depends on the model you choose.**

***

#### Lighter models, lighter requirements

Some models are small and efficient. Others are massive and hungry for memory. If you're just getting started or working on a regular laptop, here‚Äôs the rule of thumb:

* If your laptop isn‚Äôt older than **5 or 6 years** and you have at least **8GB of RAM**, you're good to go for **small models**
* You **don‚Äôt need a dedicated GPU** to get started. Small models like **TinyLLaMA** (around 638‚ÄØMB) or **Gemma 3B:1b** (around 815‚ÄØMB) can run on the **CPU**
* Things might run a bit slower on CPU, but they still work fine for basic use

***

#### What happens with bigger models?

Once you go beyond the small ones, like 13B or larger, you‚Äôll start to need more power:

* **More RAM** (16GB or more recommended)
* Ideally a **dedicated GPU** with enough **VRAM** (8GB or more)
* A system that can handle heavier memory loads without slowing down

In short, the bigger the model, the more your system needs to keep up.

***

#### So‚Ä¶ will it work?

Most likely, yes, especially if you start small. XLlama won‚Äôt break anything. If a model is too large, it either won‚Äôt load or will run very slowly.

Just save your work first ‚Äî especially any open Excel files ‚Äî and give it a try. Better safe than sorry üòâ
